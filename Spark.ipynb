{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01f14e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, BooleanType, DoubleType\n",
    "from pyspark.sql.functions import from_json, col, avg, abs, size, split, count, when, rank\n",
    "from pyspark.sql.window import Window\n",
    "import findspark\n",
    "import pandas as pd\n",
    "import time\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36ef1182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection to SQLite database\n",
    "conn = sqlite3.connect(r'C:\\databases\\kpi_data.db')\n",
    "# Create a cursor object to execute SQL commands\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c571cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark environment\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3145bde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of required JAR files for Spark-Kafka integration\n",
    "jar_files = [\"C:\\\\Spark\\\\jars\\\\spark-sql-kafka-0-10_2.12-3.5.1.jar\",\n",
    "             \"C:\\\\spark\\\\jars\\\\kafka-clients-3.7.1.jar\",\n",
    "             \"C:\\\\Spark\\\\jars\\\\commons-pool2-2.11.1.jar\",\n",
    "             \"C:\\\\Spark\\\\jars\\\\spark-token-provider-kafka-0-10_2.12-3.5.1.jar\",\n",
    "             \"C:\\\\Spark\\\\jars\\\\spark-streaming-kafka-0-10-assembly_2.12-3.5.1.jar\"]\n",
    "\n",
    "# Combine all JAR files into a single string\n",
    "jars = \",\".join(jar_files)\n",
    "\n",
    "# Initialize Spark Session with JAR files\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaSparkConsumer\") \\\n",
    "    .config(\"spark.jars\", jars) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d40fc9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the JSON data coming from Kafka\n",
    "schema = StructType([\n",
    "    StructField(\"_id\", StructType([\n",
    "        StructField(\"_data\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"operationType\", StringType(), True),\n",
    "    StructField(\"clusterTime\", StructType([\n",
    "        StructField(\"$timestamp\", StructType([\n",
    "            StructField(\"t\", LongType(), True),\n",
    "            StructField(\"i\", LongType(), True)\n",
    "        ]), True)\n",
    "    ]), True),\n",
    "    StructField(\"wallTime\", StructType([\n",
    "        StructField(\"$date\", LongType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"fullDocument\", StructType([\n",
    "        StructField(\"_id\", StructType([\n",
    "            StructField(\"$oid\", StringType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"rated\", BooleanType(), True),\n",
    "        StructField(\"created_at\", DoubleType(), True),\n",
    "        StructField(\"last_move_at\", DoubleType(), True),\n",
    "        StructField(\"turns\", LongType(), True),\n",
    "        StructField(\"victory_status\", StringType(), True),\n",
    "        StructField(\"winner\", StringType(), True),\n",
    "        StructField(\"increment_code\", StringType(), True),\n",
    "        StructField(\"white_id\", StringType(), True),\n",
    "        StructField(\"white_rating\", LongType(), True),\n",
    "        StructField(\"black_id\", StringType(), True),\n",
    "        StructField(\"black_rating\", LongType(), True),\n",
    "        StructField(\"moves\", StringType(), True),\n",
    "        StructField(\"opening_eco\", StringType(), True),\n",
    "        StructField(\"opening_name\", StringType(), True),\n",
    "        StructField(\"opening_ply\", LongType(), True)\n",
    "    ]), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d765d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a streaming DataFrame to read from the Kafka topic and parse the JSON data\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"chess-.chess.games\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING) as json_string\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea939270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'fullDocument' field and its nested fields into a DataFrame\n",
    "chess_streaming_df = df.select(from_json(col(\"json_string\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.fullDocument.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4561adab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable to hold the accumulated Spark DataFrame\n",
    "chess_df = None\n",
    "\n",
    "# Function to process each batch and append to the global DataFrame\n",
    "def process_batch(batch_df, epoch_id):\n",
    "    global chess_df\n",
    "    if chess_df is None:\n",
    "        chess_df = batch_df\n",
    "    else:\n",
    "        chess_df = global_spark_df.union(chess_df)\n",
    "\n",
    "# Write the output of the stream using foreachBatch to process each batch\n",
    "query = chess_streaming_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start()\n",
    "\n",
    "# Wait for some time to accumulate data\n",
    "time.sleep(3)\n",
    "\n",
    "# Stop the stream after collecting data\n",
    "query.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a042097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns before removing unused columns:  17\n",
      "Number of columns after removing unused columns:  11\n"
     ]
    }
   ],
   "source": [
    "# Remove unused columns and print column count before and after removal\n",
    "print(\"Number of columns before removing unused columns: \", len(chess_df.columns))\n",
    "chess_df = chess_df.drop(\"created_at\",\"last_move_at\",\"rated\",\"white_id\",\"black_id\",\"opening_eco\")\n",
    "print(\"Number of columns after removing unused columns: \", len(chess_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ffeaa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of games before deleting rows with missing values:  20058\n",
      "Number of games after deleting rows with missing values:  20058\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values and print counts before and after dropping\n",
    "print(\"Number of games before deleting rows with missing values: \",chess_df.count())\n",
    "chess_df = chess_df.na.drop()\n",
    "print(\"Number of games after deleting rows with missing values: \",chess_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f9f682f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of games before removing duplicates:  20058\n",
      "Number of games after removing duplicates:  19113\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate games based on the 'id' column and print counts before and after removal\n",
    "print(\"Number of games before removing duplicates: \", chess_df.count())\n",
    "chess_df = chess_df.dropDuplicates([\"id\"])\n",
    "print(\"Number of games after removing duplicates: \", chess_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81561233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of games\n",
    "total_games = chess_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6da12460",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "# Calculate the percentage of games won by the white pieces\n",
    "value = (chess_df.filter(col(\"winner\") == \"white\").count()/total_games)*100\n",
    "data.append((\"White winning %\",value))\n",
    "\n",
    "# Calculate the percentage of games won by the black pieces\n",
    "value = (chess_df.filter(col(\"winner\") == \"black\").count()/total_games)*100\n",
    "data.append((\"Black winning %\",value))\n",
    "\n",
    "# Calculate the percentage of games that ended in a draw\n",
    "value = (chess_df.filter(col(\"winner\") == \"draw\").count()/total_games)*100\n",
    "data.append((\"Draw %\",value))\n",
    "\n",
    "# Calculate the average number of turns per game\n",
    "value = chess_df.select(avg(\"turns\")).collect()[0][0]\n",
    "data.append((\"Average Turns\",value))\n",
    "\n",
    "# Calculate the average number of moves in the opening phase of the game \n",
    "value  = chess_df.select(avg(\"opening_ply\")).collect()[0][0]\n",
    "data.append((\"Average Opening Ply\",value))\n",
    "\n",
    "# Loop through different rating difference thresholds (100, 200, 300)\n",
    "# For each threshold, calculate the percentage of games won by the lower-rated player \n",
    "# where the rating difference is greater than the threshold.\n",
    "for x in [100,200,300]:\n",
    "    value = (\n",
    "    (chess_df.filter((col(\"white_rating\") - col(\"black_rating\") > x) & (col(\"winner\") == \"black\")).count()\n",
    "     + chess_df.filter((col(\"black_rating\") - col(\"white_rating\") > x) & (col(\"winner\") == \"white\")).count())\n",
    "    / chess_df.filter(abs(col(\"white_rating\") - col(\"black_rating\")) > x).count())\n",
    "    value *= 100\n",
    "    data.append((f\"% of games won by lower rated Player (when rating difference > {x})\",value))\n",
    "\n",
    "# Calculate the percentage of games that ended with scholar's mate (a specific checkmate pattern)\n",
    "value = (chess_df.filter((size(split(col(\"moves\"),\" \")) == 7) & (col(\"winner\") == \"white\") & (col(\"victory_status\") == \"mate\"))\n",
    "         .count()/total_games)*100\n",
    "data.append((\"% of games that ended with scholar mate\",value))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ad2c71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|            KPI Name|              Value|\n",
      "+--------------------+-------------------+\n",
      "|     White winning %| 49.939831528279186|\n",
      "|     Black winning %| 45.414116046669804|\n",
      "|              Draw %|  4.646052425051012|\n",
      "|       Average Turns| 60.513838748495786|\n",
      "| Average Opening Ply|  4.815779835713912|\n",
      "|% of games won by...|  24.98313903073514|\n",
      "|% of games won by...| 19.004676018704075|\n",
      "|% of games won by...| 15.685706001739636|\n",
      "|% of games that e...|0.10987286140323341|\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame for the calculated KPIs\n",
    "schema = StructType([\n",
    "    StructField(\"KPI Name\",StringType(),True),\n",
    "    StructField(\"Value\",DoubleType(),True)\n",
    "])\n",
    "kpi_df = spark.createDataFrame(data, schema = schema)\n",
    "\n",
    "# Display the DataFrame\n",
    "kpi_df.show()\n",
    "\n",
    "# Convert the DataFrame to a Pandas DataFrame for insertion into SQLite\n",
    "pandas_df = kpi_df.toPandas()\n",
    "\n",
    "# Create a table (named table1) in the SQLite database (if it doesn't exist) and insert the DataFrame\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS table1 (\n",
    "    kpi_id INTEGER PRIMARY KEY,\n",
    "    kpi_name TEXT,\n",
    "    kpi_value Double\n",
    ")\n",
    "''')\n",
    "pandas_df.to_sql('table1', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f23dc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|  major_opening_name|           Usage %|\n",
      "+--------------------+------------------+\n",
      "|    Sicilian Defense|12.938837440485534|\n",
      "|      French Defense| 6.581907602155601|\n",
      "|   Queen's Pawn Game| 5.294825511432009|\n",
      "|        Italian Game| 4.844869983780673|\n",
      "|    King's Pawn Game| 4.557107727724585|\n",
      "|           Ruy Lopez| 4.248417307591692|\n",
      "|     English Opening| 3.657196672421912|\n",
      "|Scandinavian Defense| 3.599644221210694|\n",
      "|   Caro-Kann Defense|2.9247109297336893|\n",
      "|         Scotch Game|2.3073300894679014|\n",
      "|Queen's Gambit De...|1.9724794642389996|\n",
      "|   Four Knights Game| 1.799822110605347|\n",
      "|Van't Kruijs Opening| 1.789358028566944|\n",
      "|         Indian Game|1.6323967979908962|\n",
      "|    Bishop's Opening| 1.621932715952493|\n",
      "|   Zukertort Opening| 1.559148223722074|\n",
      "|    Philidor Defense|1.3760267880500183|\n",
      "|Queen's Gambit Re...| 1.276618008685188|\n",
      "|        Russian Game| 1.266153926646785|\n",
      "|Queen's Gambit Ac...| 1.266153926646785|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate usage % of each major opening and display the result\n",
    "chess_df = chess_df.withColumn(\"major_opening_name\", split(col(\"opening_name\"),\":\")[0])\n",
    "kpi_df = chess_df.groupBy(\"major_opening_name\").count()\n",
    "kpi_df = kpi_df.withColumn(\"Usage %\",(col(\"count\")/total_games)*100)\n",
    "kpi_df = kpi_df.select(\"major_opening_name\",\"Usage %\")\n",
    "kpi_df = kpi_df.orderBy(col(\"Usage %\").desc())\n",
    "kpi_df.show()\n",
    "\n",
    "# Convert the DataFrame to a Pandas DataFrame for insertion into SQLite\n",
    "pandas_df = kpi_df.toPandas()\n",
    "\n",
    "# Create a table (named table2) in the SQLite database (if it doesn't exist) and insert the DataFrame\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS table2 (\n",
    "    kpi_id INTEGER PRIMARY KEY,\n",
    "    kpi_name TEXT,\n",
    "    kpi_value Double\n",
    ")\n",
    "''')\n",
    "pandas_df.to_sql('table2', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d485eecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+------------------+------------------+\n",
      "|increment_code|           Usage %|       Time Loss %|     Resignation %|\n",
      "+--------------+------------------+------------------+------------------+\n",
      "|          10+0|  38.4868937372469| 9.883088635127788| 54.74442631865144|\n",
      "|          15+0| 6.581907602155601|  8.34658187599364| 55.16693163751988|\n",
      "|         15+15| 4.295505676764506|3.6540803897685747|56.881851400730824|\n",
      "|           5+5|  3.78276565688275| 9.266943291839558|56.293222683264176|\n",
      "|           5+8| 3.547323811018678| 8.702064896755163| 56.78466076696165|\n",
      "|           8+0|2.9456390938104953|17.761989342806395|49.733570159857905|\n",
      "|          10+5|2.7939099042536495| 7.490636704119851| 58.80149812734082|\n",
      "|         15+10| 2.249777638256684| 4.651162790697675| 64.88372093023256|\n",
      "|          20+0| 2.239313556218281| 5.607476635514018| 52.10280373831776|\n",
      "|          30+0|1.8835347669125726|3.3333333333333335| 59.72222222222222|\n",
      "|         10+10|1.6951812902213155|  4.62962962962963|56.481481481481474|\n",
      "|          15+5|1.5643802647412757|  5.68561872909699| 56.52173913043478|\n",
      "|           7+2|1.1353529011667451|10.599078341013826| 43.77880184331797|\n",
      "|          10+2|0.9417673834562863| 8.333333333333332|48.888888888888886|\n",
      "|         30+30|0.8371265630722544|             1.875|             71.25|\n",
      "|          5+10|0.7952702349186418| 7.894736842105263| 52.63157894736842|\n",
      "|          10+3|0.7586459477842306|11.724137931034482|55.172413793103445|\n",
      "|          25+0|0.7377177837074242|  8.51063829787234|57.446808510638306|\n",
      "|          10+8|0.6435410453617957| 7.317073170731707| 48.78048780487805|\n",
      "|         20+10|0.5284361429393606| 7.920792079207921| 56.43564356435643|\n",
      "+--------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group the  chess_df DataFrame by 'increment_code' and count the number of games for each group\n",
    "kpi_df = chess_df.groupBy(\"increment_code\").agg(count(\"*\").alias(\"Total Games Count\"))\n",
    "\n",
    "# Filter the DataFrame to include only games that ended with a loss due to time running out ('outoftime')\n",
    "# Group the resulting DataFrame by 'increment_code' and count the number of time losses for each group\n",
    "time_loss_df = chess_df.filter(col(\"victory_status\") == \"outoftime\")\n",
    "time_loss_df = time_loss_df.groupBy(\"increment_code\").agg(count(\"*\").alias(\"Time Loss Count\"))\n",
    "\n",
    "# Filter the DataFrame to include only games that ended with resignation ('resign')\n",
    "# Group the resulting DataFrame by 'increment_code' and count the number of resignations for each group\n",
    "resignation_df = chess_df.filter(col(\"victory_status\") == \"resign\")\n",
    "resignation_df = resignation_df.groupBy(\"increment_code\").agg(count(\"*\").alias(\"Resignations Count\"))\n",
    "\n",
    "# This combines the total games, time loss counts, and resignation count into a single DataFrame\n",
    "# (by performing join operations according to 'increment_code column')\n",
    "kpi_df = kpi_df.join(time_loss_df,on = \"increment_code\",how = \"inner\")\n",
    "kpi_df = kpi_df.join(resignation_df,on = \"increment_code\",how = \"inner\")\n",
    "\n",
    "# Calculate the usage percentage for each increment code (i.e., the percentage of games using that increment code)\n",
    "kpi_df = kpi_df.withColumn(\"Usage %\", (col(\"Total Games Count\")/total_games)*100)\n",
    "\n",
    "# Calculate the percentage of games that ended due to time running out for each increment code\n",
    "kpi_df = kpi_df.withColumn(\"Time Loss %\", (col(\"Time Loss Count\")/col('Total Games Count')*100))\n",
    "\n",
    "# Calculate the percentage of games that ended with resignation for each increment code\n",
    "kpi_df = kpi_df.withColumn(\"Resignation %\", (col(\"Resignations Count\")/col('Total Games Count')*100))\n",
    "\n",
    "# Select only the relevant columns: 'increment_code', 'Usage %', 'Time Loss %', and 'Resignation %'\n",
    "kpi_df = kpi_df.select(\"increment_code\",\"Usage %\",\"Time Loss %\",\"Resignation %\")\n",
    "\n",
    "# Order the resulting DataFrame by 'Usage %' in descending order, so that the most used increment codes are listed first\n",
    "kpi_df = kpi_df.orderBy(col(\"Usage %\").desc())\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "kpi_df.show()\n",
    "\n",
    "# Convert the DataFrame to a Pandas DataFrame for insertion into SQLite\n",
    "pandas_df = kpi_df.toPandas()\n",
    "\n",
    "# Create a table (named table3) in the SQLite database (if it doesn't exist) and insert the DataFrame\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS table3 (\n",
    "    kpi_id INTEGER PRIMARY KEY,\n",
    "    kpi_name TEXT,\n",
    "    kpi_value Double\n",
    ")\n",
    "''')\n",
    "pandas_df.to_sql('table3', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "687ff3c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a function to classify ratings into ranges.\n",
    "# This function creates labels such as \"rookie\", \"beginner\", \"intermediate\", etc., based on the player's rating.\n",
    "def rating_range(rating):\n",
    "    return when(col(rating) <= 1100, \"rookie - <= 1100\") \\\n",
    "        .when((col(rating) > 1100) & (col(rating) <= 1600), \"beginner - (1100, 1600]\") \\\n",
    "        .when((col(rating) > 1600) & (col(rating) <= 2000), \"intermediate - (1600, 2000]\") \\\n",
    "        .when((col(rating) > 2000) & (col(rating) <= 2300), \"advance - (2000, 2300]\") \\\n",
    "        .otherwise(\"expert - > 2300\")\n",
    "\n",
    "# Add new columns to the DataFrame representing the rating ranges for both white and black players.\n",
    "processed_chess_df = chess_df.withColumn(\"White Rating Range\", rating_range(\"white_rating\"))\n",
    "processed_chess_df = processed_chess_df.withColumn(\"Black Rating Range\", rating_range(\"black_rating\"))\n",
    "\n",
    "# Add a 'Game Level' column, which stores the rating range if both players are in the same range.\n",
    "# If they are not, the value is set to None.\n",
    "processed_chess_df = processed_chess_df.withColumn(\"Game Level\",\n",
    "    when(col(\"White Rating Range\") == col(\"Black Rating Range\"), col(\"White Rating Range\"))\n",
    "    .otherwise(None)\n",
    ")\n",
    "\n",
    "# Remove rows where 'Game Level' is null (i.e., games where the players' ratings are in different ranges).\n",
    "processed_chess_df = processed_chess_df.dropna()\n",
    "\n",
    "# Calculate the total number of games that have a defined 'Game Level'.\n",
    "total_defined_games = processed_chess_df.count()\n",
    "\n",
    "# Aggregate the number of games for each 'Game Level'.\n",
    "kpi_df = processed_chess_df.groupBy(\"Game Level\").agg(count(\"*\").alias(\"Total Games Count\"))\n",
    "\n",
    "# Filter the DataFrame to include only the games where the victory status was \"outoftime\"\n",
    "# then group by 'Game Level' and count the number of such games for every Game Level.\n",
    "time_loss_df = processed_chess_df.filter(col(\"victory_status\") == \"outoftime\").groupBy(\"Game Level\").agg(count(\"*\").alias(\"Time Loss Count\"))\n",
    "\n",
    "# Filter the DataFrame to include only the games where the victory status was \"resign\"\n",
    "# then group by 'Game Level' and count the number of such games for every Game Level.\n",
    "resignation_df = processed_chess_df.filter(col(\"victory_status\") == \"resign\").groupBy(\"Game Level\").agg(count(\"*\").alias(\"Resignations Count\"))\n",
    "\n",
    "# Filter the DataFrame to include only the games where the victory status was \"draw\"\n",
    "# then group by 'Game Level' and count the number of such games for every Game Level.\n",
    "draw_df = processed_chess_df.filter(col(\"victory_status\") == \"draw\").groupBy(\"Game Level\").agg(count(\"*\").alias(\"Draws Count\"))\n",
    "\n",
    "# Filter the DataFrame to include only the games where the victory status was \"mate\"\n",
    "# then group by 'Game Level' and count the number of such games for every Game Level.\n",
    "mate_df = processed_chess_df.filter(col(\"victory_status\") == \"mate\").groupBy(\"Game Level\").agg(count(\"*\").alias(\"Mates Count\"))\n",
    "\n",
    "# Group the DataFrame by 'Game Level' and 'major_opening_name', then count the occurrences\n",
    "# of each opening within each game level.\n",
    "grouped_df = processed_chess_df.groupBy(\"Game Level\", \"major_opening_name\").count()\n",
    "\n",
    "# Define a window specification that partitions the data by 'Game Level' and orders\n",
    "# the openings within each level by their frequency in descending order.\n",
    "window_spec = Window.partitionBy(\"Game Level\").orderBy(col(\"count\").desc())\n",
    "\n",
    "\n",
    "# Apply ranking to the openings within each game level based on their frequency,\n",
    "# then filter to keep only the most common (highest ranked) opening for each game level.\n",
    "# The resulting DataFrame will contain the most popular opening for each game level.\n",
    "ranked_df = grouped_df.withColumn(\"rank\", rank().over(window_spec))\n",
    "most_common_opening_df = ranked_df.filter(col(\"rank\") == 1).select(\"Game Level\", \"major_opening_name\") \\\n",
    "                    .withColumnRenamed(\"major_opening_name\", \"Most Common Opening\")\n",
    "\n",
    "# Join all the aggregated dataframes (total games, time losses, resignations, draws, mates, and most common opening)\n",
    "# based on the 'Game Level' column.\n",
    "kpi_df = kpi_df.join(time_loss_df, on=\"Game Level\", how=\"inner\")\n",
    "kpi_df = kpi_df.join(resignation_df, on=\"Game Level\", how=\"inner\")\n",
    "kpi_df = kpi_df.join(draw_df, on=\"Game Level\", how=\"inner\")\n",
    "kpi_df = kpi_df.join(mate_df, on=\"Game Level\", how=\"inner\")\n",
    "kpi_df = kpi_df.join(most_common_opening_df, on=\"Game Level\", how=\"inner\")\n",
    "\n",
    "# Calculate the percentage of games for each 'Game Level' relative to the total number of defined games.\n",
    "kpi_df = kpi_df.withColumn(\"% Of Games\", (col(\"Total Games Count\") / total_defined_games) * 100)\n",
    "\n",
    "# Calculate the percentage of games that ended due to time running out within each 'Game Level'.\n",
    "kpi_df = kpi_df.withColumn(\"Time Loss %\", (col(\"Time Loss Count\") / col(\"Total Games Count\")) * 100)\n",
    "\n",
    "# Calculate the percentage of games that ended in resignation within each 'Game Level'.\n",
    "kpi_df = kpi_df.withColumn(\"Resignation %\", (col(\"Resignations Count\") / col(\"Total Games Count\")) * 100)\n",
    "\n",
    "# Calculate the percentage of games that ended in a draw within each 'Game Level'.\n",
    "kpi_df = kpi_df.withColumn(\"Draw %\", (col(\"Draws Count\") / col(\"Total Games Count\")) * 100)\n",
    "\n",
    "# Calculate the percentage of games that ended with a checkmate within each 'Game Level'.\n",
    "kpi_df = kpi_df.withColumn(\"Mate %\", (col(\"Mates Count\") / col(\"Total Games Count\")) * 100)\n",
    "\n",
    "# Select the desired columns and order the data by the percentage of games in descending order.\n",
    "kpi_df = kpi_df.select(\"Game Level\", \"% Of Games\", \"Time Loss %\", \"Resignation %\", \"Draw %\", \"Mate %\", \"Most Common Opening\") \\\n",
    "    .orderBy(col(\"% Of Games\").desc())\n",
    "\n",
    "# Convert the DataFrame to a Pandas DataFrame for insertion into SQLite\n",
    "pandas_df = kpi_df.toPandas()\n",
    "\n",
    "# Create a table (named table4 in the SQLite database (if it doesn't exist) and insert the DataFrame\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS table4 (\n",
    "    kpi_id INTEGER PRIMARY KEY,\n",
    "    kpi_name TEXT,\n",
    "    kpi_value Double\n",
    ")\n",
    "''')\n",
    "pandas_df.to_sql('table4', conn, if_exists='replace', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
